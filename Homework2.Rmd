---
title: "Homework 2"
subtitle: <center> <h1>Simple Linear Regression Model Assumptions</h1> </center>
author: <center> Tanner Markham <center>
output: html_document
---

<style type="text/css">
h1.title {
  font-size: 40px;
  text-align: center;
}
</style>

```{r setup, include=FALSE}
library(tidyverse)
library(readr)
library(ggfortify)
library(car)
```

## Data and Description

One key component of determining appropriate speed limits is the amount of distance that is required to stop at a given speed. For example, in residential neighborhoods, when pedestrians are commonly in the roadways, it is important to be able to stop in a very short distance to ensure pedestrian safety. The speed of vehicles may be useful for determining the distance required to stop at that given speed, which can aid public officials in determining speed limits.

The Stopping Distance data set compares the **distance (column 2)** (in feet) required for a car to stop on a certain rural road against the **speed (column 1)** (MPH) of the car. Download the StoppingDistance.txt file from Canvas, and put it in the same folder as this R Markdown file.

#### 0. Replace the text "< PUT YOUR NAME HERE >" (above next to "author:") with your full name.

#### 1. Read in the data set, and call the tibble "stop". Print a summary of the data and make sure the data makes sense.

```{r}
# <your code here>
stop <- read_table("StoppingDistance.txt")

summary(stop)
```

#### 2. Create a scatterplot of the data with variables on the appropriate axes (think about which variable makes the most sense to be the response). Make you plot look professional (make sure the axes labels are descriptive, the plot is square, etc.).

```{r, fig.align='center'}
# <your code here>
stop_plot <- ggplot(data = stop) +
  geom_point(mapping = aes(x = Speed, y = Distance)) +
  theme(aspect.ratio = 1) +
  xlab('Speed (MPH)') +
  ylab('Distance (in feet)')

stop_plot

```

#### 3. Briefly describe the relationship between Speed and Distance. (Hint: you should use 2 or 3 key words.)

With a correlation of about .94, there is a strong linear relationship between speed and distance. When speed increases in MPH, average distance in feet also increases.  

#### 4. Add the OLS regression line to the scatterplot you created in question 2 (note: if you receive a warning about rows with missing values, you may need to adjust an axis limit using `scale_y_continuous(limits = c(###, ###))`).

```{r, fig.align='center'}
stop_plot +
  geom_smooth(mapping = aes(x = Speed, y = Distance),
              method = 'lm',
              se = FALSE)

```

#### 5. (a) Apply linear regression to the data(no transformations). (b) Print out a summary of the results from the `lm` function. (c) Save the residuals and fitted values to the `stop` tibble.

```{r}
# <your code here>
model <- lm(Distance ~ Speed, data = stop)
summary(model)

stop$Fitted <- model$fitted.values
stop$Residuals <- model$residuals

```

#### 6. Mathematically write out the fitted simple linear regression model for this data set using the coefficients you found above. Do not use "x" and "y" in your model - use variable names that are fairly descriptive, and do not use matrix notation.

$\hat{Miles_i} = -20.13094 +$ $3.14138MPH_i$ 



### Questions 7-12 involve using diagnostics to determine if the linear regression assumptions are met. For each assumption, (1) perform appropriate diagnostics to determine if the assumption is violated, and (2) explain whether or not you think the assumption is violated and why you think that.

#### 7. (L) X vs Y is linear (use at least two diagnostic tools)

```{r, fig.align='center'}
# <your code here>
stop_plot
res_vs_fitted <- autoplot(model, which = 1, ncol = 1, nrow = 1)
res_vs_fitted

```

Based on both the scatter plot and the residual-fitted plot, I don't believe that there is a linear relationship between Distance vs Speed. Looking at the scatter plot, there are values at higher speeds that are much further from the trend line than at lower speeds. On the residual-fitted plot, there is a parabolic curve as you go across the fitted values instead of a straight line across.

#### 8. (I) The residuals are independent (no diagnostic tools - just think about how the data was collected and briefly write your thoughts)

I believe the residuals are independent because the observations were done individually, and the order in which those observations are noted doesn't affect the overall analysis.

#### 9. (N) The residuals are normally distributed and centered at zero (use at least three diagnostic tools)

```{r, fig.align='center'}
# <your code here>
q_q <- autoplot(model, which = 2, nrow = 1, ncol = 1)

stop_hist <- ggplot(data = stop) +
  geom_histogram(aes(x = Residuals, y = ..density..),
                 binwidth = 10) +
  stat_function(fun = dnorm,
                color = 'red',
                size = 2,
                args = list(mean = mean(stop$Residuals),
                            sd = sd(stop$Residuals))) +
  theme(aspect.ratio = 1)

stop_box <- ggplot(data = stop) +
  geom_boxplot(aes(y = Residuals)) +
  theme(aspect.ratio = 1)


q_q
stop_hist
stop_box

```

Based on the plots above, I feel comfortable with the assumption that residuals are normally distributed and centered at zero. On the q_q plot, the values are all centered around the center line enough, with the exception of a few outliers. The histogram is pretty normally distributed, and the box plot is also mostly centered at zero with the median generally in the center of the box. 

#### 10. (E) The residuals have equal/constant variance across all values of X (use two diagnostic tools)

```{r, fig.align='center'}
# <your code here>
res_vs_fitted
grp <- as.factor(c(rep("lower", floor(dim(stop)[1] / 2)), 
                   rep("upper", ceiling(dim(stop)[1] / 2))))
leveneTest(unlist(stop[order(stop$Speed), "Residuals"]) ~ grp, 
           center = median)
```

Looking at the residual vs fitted model, the vertical distance between points is not equal/constant variance across the fitted values. Additionally, the results of the Brown-Forsythe test gives a very low p-value, which is enough evidence to reject the null hypothesis that states there is equal/constant variance.

#### 11. (A) The model describes all observations (i.e., there are no influential points) (use at least four diagnostic tools)

```{r, fig.align='center'}

# Cook's Distance
stop$cooksd <- cooks.distance(model)

cook <- ggplot(data = stop) + 
  geom_point(mapping = aes(x = as.numeric(rownames(stop)), 
                           y = cooksd)) +
  ylab("Cook's Distance") +
  xlab("Observation Number") +
  geom_hline(mapping = aes(yintercept = 4 / length(cooksd)),
             color = "red", 
             linetype = "dashed") +
  theme(aspect.ratio = 1)

stop %>% 
  mutate(rowNum = row.names(stop)) %>%  # save original row numbers 
  filter(cooksd > 4 / length(cooksd)) %>%  # select potential outliers
  arrange(desc(cooksd))


# DFBETAS
stop$dfbetas_weight <- as.vector(dfbetas(model)[, 2])

dfbeta <- ggplot(data = stop) + 
  geom_point(mapping = aes(x = as.numeric(rownames(stop)), 
                           y = abs(dfbetas_weight))) +
  ylab("Absolute Value of DFBETAS for Speed") +
  xlab("Observation Number") +
  geom_hline(mapping = aes(yintercept = 2 / sqrt(length(dfbetas_weight))),
             color = "red", 
             linetype = "dashed") + 
  theme(aspect.ratio = 1)

stop %>% 
  mutate(rowNum = row.names(stop)) %>%  # save original row numbers 
  filter(abs(dfbetas_weight) > 2 / 
           sqrt(length(rownames(stop)))) %>%  # select potential influential pts
  arrange(desc(abs(dfbetas_weight)))


# DFFITS
stop$dffits <- dffits(model)

dffits <- ggplot(data = stop) + 
  geom_point(mapping = aes(x = as.numeric(rownames(stop)), 
                           y = abs(dffits))) +
  ylab("Absolute Value of DFFITS for Distance") +
  xlab("Observation Number") +
  geom_hline(mapping = aes(yintercept = 2 * sqrt(length(model$coefficients) /
                                                   length(dffits))),
             color = "red", 
             linetype = "dashed") +
  theme(aspect.ratio = 1)

stop %>% 
  mutate(rowNum = row.names(stop)) %>%  # save original row numbers 
  # select potential influential pts
  filter(abs(dffits) > 2 * sqrt(length(model$coefficients) / 
                                  length(dffits))) %>%
  arrange(desc(abs(dffits)))

stop_box
cook
dfbeta
dffits

```

Based on plots and values generated from the various tests. I don't believe that this data is free of influential points. There are observations that show in the cook, dfbeta, and dffits tests that suggest strongly that they are influential points.

#### 12. (R) Additional predictor variables are not required (no diagnostic tools - just think about the variables you have and if there are other variables you think would help predict the response)

I believe that additional predictor variables are required. Factors such as the weight of the car, the drivetrain, and road conditions.

#### 13. Based on your analysis of the diagnostic measures, briefly discuss why this simple linear regression model on the raw data (not transformed) is *not* appropriate.

Based on all of the diagnostics, this simple linear regression model is not appropriate for this data as it stands. Although some assumptions are met, there are enough which aren't met to warrant some transformations.

#### 14. Fix the model by making any necessary transformations. Justify the transformation you chose in words (why did you choose to transform just x, just y, or both?). (Note: if boxCox(mod) throws an error, replace mod with the formula for the linear model, y ~ x.) (Note: you will most likely need to repeat questions 14 and 18 until you are satisfied with the transformation you chose. Only then should you fill out this section - I only want to see the model you end up choosing, not all of your attempted models.)

```{r, fig.align='center'}
bc <- boxCox(stop$Distance ~ stop$Speed)
bc$x[which.max(bc$y)]
stop$y_trans <- sqrt(stop$Distance)

updated_model <- lm(y_trans ~ Speed, data = stop)
summary(updated_model)
stop$Fitted_trans <- updated_model$fitted.values
stop$Residuals_trans <- updated_model$residuals

```

Because of the violated assumptions, I determined a transformation was necessary. By starting with a Box Cox value of .42, and through some trial and error, a transformation of the square root of Y was most effective. I explored log transformations of both X and Y, but this created stronger violations of the data. Simply taking the square root of Y fixed all of the assumptions without any transformations on X.

### Now, in Questions 15-18, re-check your transformed model and verify that the assumptions (the assumptions that were addressed in the questions above) are met. Provide a brief discussion about how each of the previously violated assumptions are now satisfied. Also, provide the code you used to assess adherence to the assumptions. (Note that transforming will not change your responses about (I) the residuals being independent and (R) additional predictor variables not being required, so we will skip these assumptions here.)

#### 15. (L) Linearity (use at least two diagnostic tools)

```{r, fig.align='center'}
# <your code here>
trans_stop_plot <- ggplot(data = stop) +
  geom_point(mapping = aes(x = Speed, y = y_trans)) +
  theme(aspect.ratio = 1) +
  xlab('Speed (MPH)') +
  ylab('Square Root of Distance (in feet)')

trans_stop_plot

trans_res_vs_fitted <- autoplot(updated_model, which = 1, ncol = 1, nrow = 1)
trans_res_vs_fitted
```

Based on the transformed plots, the data appears to be linear. The scatter plot follows a much straighter pattern. The blue line across the residual vs fitted plot is also almost perfectly straight.

#### 16.  (N) The residuals are normally distributed and centered at zero (use at least three diagnostic tools)

```{r, fig.align='center'}
# <your code here>
trans_q_q <- autoplot(updated_model, which = 2, nrow = 1, ncol = 1)

trans_hist <- ggplot(data = stop) +
  geom_histogram(aes(x = Residuals_trans, y = ..density..),
                 binwidth = 2) +
  stat_function(fun = dnorm,
                color = 'red',
                size = 2,
                args = list(mean = mean(stop$Residuals_trans),
                            sd = sd(stop$Residuals_trans))) +
  theme(aspect.ratio = 1)

trans_box <- ggplot(data = stop) +
  geom_boxplot(aes(y = Residuals_trans)) +
  theme(aspect.ratio = 1)

trans_q_q
trans_hist
trans_box

```

The transformed data appears to fit the assumption of being normally distributed and centered at zero. The Q Q plot values are more tightly centered around the center line than before the transformation, and the box plots and histogram both show a normal distribution centered around zero.

#### 17. (E) The residuals have equal/constant variance across all values of X (use two diagnostic tools)

```{r, fig.align='center'}
# <your code here>
trans_res_vs_fitted
grp_trans <- as.factor(c(rep("lower", floor(dim(stop)[1] / 2)), 
                   rep("upper", ceiling(dim(stop)[1] / 2))))
leveneTest(unlist(stop[order(stop$Speed), "Residuals_trans"]) ~ grp_trans, 
           center = median)
```

After the transformation, this model fits the equal/constant variance across X assumption. The residuals vs fitted plot shows no patterns and has a pretty equal vertical distance between points. The Brown-Forsyth test also returns a high p-value, meaning there isn't enough evidence to reject the hypothesis of homoscedasticity.

#### 18. (A) The model describes all observations (i.e., there are no influential points) (use at least four diagnostic tools)

```{r, fig.align='center'}
# <your code here>

# Cooks
stop$cooksd_trans <- cooks.distance(updated_model)

cook_trans <- ggplot(data = stop) + 
  geom_point(mapping = aes(x = as.numeric(rownames(stop)), 
                           y = cooksd_trans)) +
  ylab("Cook's Distance") +
  xlab("Observation Number") +
  geom_hline(mapping = aes(yintercept = 4 / length(cooksd_trans)),
             color = "red", 
             linetype = "dashed") +
  theme(aspect.ratio = 1)

stop %>% 
  mutate(rowNum = row.names(stop)) %>%  # save original row numbers 
  filter(cooksd_trans > 4 / length(cooksd_trans)) %>%  # select potential outliers
  arrange(desc(cooksd_trans))


#DFBETA
stop$dfbetas_weight_trans <- as.vector(dfbetas(updated_model)[, 2])

dfbeta_trans <- ggplot(data = stop) + 
  geom_point(mapping = aes(x = as.numeric(rownames(stop)), 
                           y = abs(dfbetas_weight_trans))) +
  ylab("Absolute Value of DFBETAS for Speed") +
  xlab("Observation Number") +
  geom_hline(mapping = aes(yintercept = 2 / sqrt(length(dfbetas_weight_trans))),
             color = "red", 
             linetype = "dashed") + 
  theme(aspect.ratio = 1)

stop %>% 
  mutate(rowNum = row.names(stop)) %>%  # save original row numbers 
  filter(abs(dfbetas_weight_trans) > 2 / 
           sqrt(length(rownames(stop)))) %>%  # select potential influential pts
  arrange(desc(abs(dfbetas_weight_trans)))

#DFFITS
stop$dffits_trans <- dffits(updated_model)

dffits_trans <- ggplot(data = stop) + 
  geom_point(mapping = aes(x = as.numeric(rownames(stop)), 
                           y = abs(dffits_trans))) +
  ylab("Absolute Value of DFFITS for Distance") +
  xlab("Observation Number") +
  geom_hline(mapping = aes(yintercept = 2 * sqrt(length(updated_model$coefficients) /
                                                   length(dffits_trans))),
             color = "red", 
             linetype = "dashed") +
  theme(aspect.ratio = 1)

stop %>% 
  mutate(rowNum = row.names(stop)) %>%  # save original row numbers 
  # select potential influential pts
  filter(abs(dffits_trans) > 2 * sqrt(length(updated_model$coefficients) / 
                                  length(dffits_trans))) %>%
  arrange(desc(abs(dffits_trans)))

trans_box
cook_trans
dfbeta_trans
dffits_trans
```

After transformation, this model has no influential points on the data. There are still a couple observations just above the threshold with cooks, dfbeta, and dffits, but they aren't has far from the line as observed in the untransformed data.


#### 19. Mathematically write out the fitted simple linear regression model for this data set using the coefficients you found above from your transformed model. Do not use "x" and "y" in your model - use variable names that are fairly descriptive, and do not use matrix notation.

$\sqrt{\hat{Miles_i}} = 0.9323957 +$ $0.252466MPH_i$ 

#### 20. Plot your new fitted *curve* on the scatterplot of the original data (on the original scale - not the transformed scale). Do you think this curve fits the data better than the line you previously fit?

```{r}
# <your code here>
Speed_values <- seq(min(stop$Speed), 
                     max(stop$Speed), 
                     length = 100)  
sqrt_Distance_preds <- predict(updated_model, 
                         newdata = data.frame(Speed = Speed_values))
Distance_preds <- (sqrt_Distance_preds)^2 
# Store results in a data frame for plotting
preds <- data.frame("Speed_values" = Speed_values, 
                    "Distance_preds" = Distance_preds)
# Plot the predictions on the original scale (to get a curved line)
stop_plot + 
  geom_line(data = preds, 
            mapping = aes(x = Speed_values, y = Distance_preds), 
            size = 1.5, color ="blue")
```



This curve fits the data much better than the original curve. There are values at both low and high speed/distance that are much better captured by this line.

#### 21. Briefly summarize what you learned, personally, from this analysis about the statistics, model fitting process, etc.

I've learned that adjusting a model to fit the data takes a lot more time than I imagined. It is time-consuming enough to just check for assumptions, but transforming the data and then checking assumptions all over again definitely takes time. I also learned that it's important to be well organized with your code, because you have a lot of iterations of the same plots/data and it can be easy to mix things up.

#### 22. Briefly summarize what you learned from this analysis *to a non-statistician*. Write a few sentences about (1) the purpose of this data set and analysis and (2) what you learned about this data set from your analysis. Write your response as if you were addressing a business manager (avoid using statistics jargon) and just provide the main take-aways.

The purpose of this data set and analysis is to determine how a cars speed can determine the distance required to come to a complete stop. This information is useful in deciding what to set speed limits to on the road. As perhaps it may be expected, this analysis suggests that there is a positive linear relationship between car speed in MPH and distance it takes to stop in feet, meaning that as MPH increases so too does the amount of feet it takes to stop. More specifically, the average square root of feet it takes to stop the car increases by approximately .25 feet for every increase in one mile per hour.

