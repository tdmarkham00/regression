---
title: "Homework 7"
subtitle: <center> <h1>Logistic Regression</h1> </center>
author: <center> Tanner Markham <center>
output: html_document
---

<style type="text/css">
h1.title {
font-size: 40px;
text-align: center;
}
</style>

```{r setup, include=FALSE}
# load packages here
library(tidyverse)
library(corrplot)  # for the correlation matrix
library(bestglm)  # for variable selection
library(car)  # for the VIFs
library(pROC)  # for the ROC curve
library(ROCR)  # for the color-coded ROC curve
library(ggpubr)
library(glmnet)  # for ridge, lasso, and elastic net
library(varhandle)

```

## Data and Description

Type 2 diabetes is a problem with the body that causes blood sugar levels to rise higher than normal (hyperglycemia) because the body does not use insulin properly. Specifically, the body cannot make enough insulin to keep blood sugar levels normal. Type 2 diabetes is associated with various health complications such as neuropathy (nerve damage), glaucoma, cataracts and various skin disorders. Early detection of diabetes is crucial to proper treatment so as to alleviate complications.

The data set contains information on 392 randomly selected women who are at risk for diabetes. The data set contains the following variables:

Variable  | Description
--------- | -------------
pregnant  | Number of times pregnant
glucose   | Plasma glucose concentration at 2 hours in an oral glucose tolerance test
diastolic | Diastolic blood pressure (mm Hg)
triceps   | Triceps skin fold thickness (mm)
insulin   | 2 hour serum insulin (mu U/ml)
bmi       | Body mass index ($kg/m^2$, mass in kilograms divided by height in meters-squared)
pedigree  | Numeric strength of diabetes in family line (higher numbers mean stronger history)
age       | Age
diabetes  | Does the patient have diabetes (0 if "No", 1 if "Yes")

The data can be found in the Diabetes data set on Canvas. Download Diabetes.txt, and put it in the same folder as this R Markdown file.

#### 0. Replace the text "< PUT YOUR NAME HERE >" (above next to "author:") with your full name.

#### 1. Read in the data set, call it "dia", remove the "row" column, and change the class of any categorical variables to a factor. Print a summary of the data and make sure the data makes sense. 

```{r}
# your code here
dia <- read_table('Diabetes.txt') %>% 
  select(!row) %>% 
  mutate(diabetes = as.factor(diabetes))
  
summary(dia)
```

#### 2. Explore the data. Create a correlation matrix for the covariates. *Comment on why or why not you think multicollinearity may be a problem for this data set.* 

```{r, fig.align='center'}
# your code here
dia %>% 
  select(!diabetes) %>% 
  cor() %>% 
  corrplot(type = 'upper')

```

Based on the correlation matrix, there are a few variables that look like they may cause a multicollinearity problem. Pregnant and age, insulin and glucose, and bmi and triceps all look highly correlated.

#### 3. Explore the data. Create boxplots of the response against the following predictors: glucose, bmi, pedigree, and age (4 plots in total. You may want to use the grid.arrange function from the gridExtra package to display them in a 2x2 grid). *Briefly comment on one interesting trend you observe.*

```{r, fig.align='center'}
# your code here
a <- ggplot(data = dia) +
  geom_boxplot(mapping = aes(y = glucose, x = diabetes)) +
  theme(aspect.ratio = 1) +
  coord_flip()

b <- ggplot(data = dia) +
  geom_boxplot(mapping = aes(y = bmi, x = diabetes)) +
  theme(aspect.ratio = 1) +
  coord_flip()

c <- ggplot(data = dia) +
  geom_boxplot(mapping = aes(y = pedigree, x = diabetes)) +
  theme(aspect.ratio = 1) +
  coord_flip()

d <- ggplot(data = dia) +
  geom_boxplot(mapping = aes(y = age, x = diabetes)) +
  theme(aspect.ratio = 1) +
  coord_flip()

ggarrange(a,b,c,d,ncol = 2, nrow = 2)
```

One interesting trend that I've observed is that for all of these predictors, the mean x value is higher for those with diabetes vs those without.

#### 4. Explore the data. Create jittered scatterplots of the response against the following predictors: pregnant, diastolic, triceps, insulin (4 plots in total. You may want to use the grid.arrange function from the gridExtra package to display them in a 2x2 grid). *Briefly comment on one interesting trend you observe.*

```{r, fig.align='center'}
a <- ggplot(data = dia) +
  geom_point(mapping = aes(y = diabetes, x = pregnant)) +
  geom_jitter(mapping = aes(y = diabetes, x = pregnant),
              height = 0.1) +
  theme(aspect.ratio = 1)

b <- ggplot(data = dia) +
  geom_point(mapping = aes(y = diabetes, x = diastolic)) +
  geom_jitter(mapping = aes(y = diabetes, x = diastolic),
              height = 0.1) +
  theme(aspect.ratio = 1)

c <- ggplot(data = dia) +
  geom_point(mapping = aes(y = diabetes, x = triceps)) +
  geom_jitter(mapping = aes(y = diabetes, x = triceps),
              height = 0.1) +
  theme(aspect.ratio = 1)

d <- ggplot(data = dia) +
  geom_point(mapping = aes(y = diabetes, x = insulin)) +
  geom_jitter(mapping = aes(y = diabetes, x = insulin),
              height = 0.1) +
  theme(aspect.ratio = 1)

ggarrange(a,b,c,d,ncol = 2, nrow = 2)
```

One thing I found interesting was the extreme cases of number of pregnancies were mostly found in those with diabetes. It makes me wonder how much diabetes contributes to miscarriages or something which could motivate people to keep getting pregnant until they carry the baby to delivery.

#### 5. Briefly explain why traditional multiple linear regression methods are not suitable for *this* data set. You should mention at least two of the reasons we discussed in class (*your reasons should refer to this data set (i.e. be specific, not general)*).

Traditional multiple linear regression is not suitable for this data because the response variable, diabetes, is a binary outcome instead of a continuous outcome. It's binary because we are predicting a simple true or false for having diabetes instead of some other continuous variable. In the case of OLS regression, the predicted values are not bounded to just 1 or 0, so we would be getting predicted values for diabetes that wouldn't make sense in the context of just yes or no does the person have diabetes. Because the predicted values would be off, the residuals would also be off which would cause problems for OLS regression assumptions such as normally distributed errors.

#### 6. Use a variable selection procedure to help you decide which, if any, variables to omit from the logistic regression model you will soon fit. You may choose which selection method to use (best subsets, backward, sequential replacement, LASSO, or elastic net) and which metric/criteria to use (AIC, BIC, or CV/PMSE). *Briefly justify (in a few sentences) why you chose the **method** and **metric** that you did.*


```{r, fig.align='center'}
# BIC
set.seed(123)
best_subsets <- bestglm(as.data.frame(dia %>% mutate(diabetes = unfactor(diabetes))),
                                  IC = "CV",
                                  method = "exhaustive",
                                  TopModels = 1,
                                  family = binomial)
summary(best_subsets$BestModel)
```

```{r}
# Elastic net
dia_x <- as.matrix(dia[, 1:8]) # predictor variables
dia_y <- unfactor(unlist(dia[, 9])) # response variable
# use cross validation to pick the "best" (based on MSE) lambda

dia_elastic_cv <- cv.glmnet(x = dia_x,
                                dia_y, 
                                type.measure = "mse", #pmse
                                alpha = .5)  # .5 is code for "Elastic net regression"

coef(dia_elastic_cv, s = "lambda.1se")

```

Based on the variable selection methods, I decided to go with the best subsets method model using cross validation. I tested with elastic net as well, but this method added additional variables and I decided to go with a more simple model. I tested with best subsets and cross validation because in my opinion, these combined procedures are the most robust way of exploring all possible combinations of variables and data being tested. I tested with elastic net as well just as an additional test to validate the best subsets. 

#### 7. Write out the logistic regression model for this data set using the covariates that you see fit. You should use parameters/Greek letters (NOT the "fitted" model using numbers...since you have not fit a model yet;) ).

$log(\frac{\pi_i}{1-pi_i})$ $=$ $\beta_0$ $+$ $\beta_1glucose_i$ $+$ $\beta_2bmi_i$ $+$ $\beta_3pedigree_i$ $+$ $\beta_4age_i$ $,where$ $\pi_i = P(diabetes_i =1|glucose_i,bmi_i,pedigree_i,age_i)$

#### 8. Fit a logistic regression model using the covariates you chose. Print a summary of the results.

```{r, fig.align='center'}
dia_lm <- glm(diabetes ~ glucose + bmi + pedigree + age, data = dia, family = binomial(link = 'logit'))
summary(dia_lm)
```




### Questions 9-13 involve using diagnostics to check the logistic regression model assumptions. For each assumption, (1) code the diagnostic(s) that I indicate (next to the assumption in parentheses) to determine if the assumption is violated, and (2) explain whether or not you think the assumption is violated and why you think that.


#### 9. The X's vs log odds are linear (monotone in probability) (Use scatterplots with smoothers)

```{r, fig.align='center'}
scatter.smooth(x = dia$glucose, y = as.numeric(dia$diabetes) - 1)
scatter.smooth(x = dia$bmi, y = as.numeric(dia$diabetes) - 1)
scatter.smooth(x = dia$pedigree, y = as.numeric(dia$diabetes) - 1)
scatter.smooth(x = dia$age, y = as.numeric(dia$diabetes) - 1)

```

All of the predictor variables have a generally constant slop without any major inflection points. This assumption is met.

#### 10. The observations are independent (no diagnostic tools - just think about how the data was collected and briefly write your thoughts)

Based on how the data is organized, I have no reason to believe that the observations violate the independence assumption. 

#### 11. The model describes all observations (i.e., there are no influential points) (Use DFFITS)

```{r, fig.align='center'}
# DFFITS
dia$dffits <- dffits(dia_lm)

ggplot(data = dia) + 
  geom_point(mapping = aes(x = as.numeric(rownames(dia)), 
                           y = abs(dffits))) +
  ylab("Absolute Value of DFFITS") +
  xlab("Observation Number") +
  geom_hline(mapping = aes(yintercept = 2 * sqrt(length(dia_lm$coefficients) /
                                                   length(dffits))),
             color = "red", 
             linetype = "dashed") +
  theme(aspect.ratio = 1)

dia %>% 
  mutate(rowNum = row.names(dia)) %>%  # save original row numbers 
  # select potential influential pts
  filter(abs(dffits) > 2 * sqrt(length(dia_lm$coefficients) / 
                                  length(dffits))) %>%
  arrange(desc(abs(dffits)))
```

I think that there is an influential point at row 236 that I would take out. 

#### 12. Additional predictor variables are not required (no diagnostic tools - just think about the variables you have and if there are other variables you think would help predict the response)

I don't know a lot about diabetes and the contributing factors, so I'd probably depend on somebody with more domain knowledge. That being said, considering the fact that I dropped half of the given variables due to multicollinearity makes me think that we're ok with the current variables.

#### 13. No multicollinearity (Use variance inflation factors)

```{r, fig.align='center'}
vif(dia_lm)
mean(vif(dia_lm))
```

Based on the low VIF values, I would say that the multicollinearity assumption is met


#### 14. Briefly comment on if all assumptions are met. If there is anything you would like to do before proceeding to statistical inference, do that here.

```{r, fig.align='center'}
dia <- dia %>% 
  mutate(rowNum = row.names(dia)) %>% 
  filter(rowNum != 236) %>% 
  select(!rowNum)
  
dia_lm <- glm(diabetes ~ glucose + bmi + pedigree + age, data = dia, family = binomial(link = 'logit'))
summary(dia_lm)
```

Other than the assumption of no influential points, all of the assumptions were met. I removed the influential point from the data and refit the model before proceeding to statistical inference.

#### 15. For the coefficient for bmi, compute (and output) the log odds ratio ($\beta_{bmi}$, pull this value from the model output), odds ratio ($\exp\{\beta_{bmi}\}$), and the odds ratio converted to a percentage ($100 \times (\exp\{\beta_{bmi}\} - 1)%$). (If you cannot view the math used in this question (and subsequent), you can see it by knitting the document.)

```{r, fig.align='center'}
# Log odds ratio
dia_lm$coefficients[3]

# Odds ratio
exp(dia_lm$coefficients[3])

# Odds ratio as percentage
100 * (exp(dia_lm$coefficients[3]) - 1)

```

#### 16. Interpret the coefficient for bmi based on the FOUR different ways we discussed in class.

*Interpretation 1:* Holding all else constant, for every one unit increase in bmi, the log odds of someone having diabetes increases by 0.072795.

*Interpretation 2:* Because 0.072795 > 0, the log odds of someone having diabetes increases as bmi increases holding all else constant.

*Interpretation 3:* Holding all else constant, as bmi increases by one unit, the odds of someone having diabetes is 1.075511 times more likely

*Interpretation 4:* Holding all else constant, as bmi increases by one unit, the odds of someone having diabetes increase by 7.551053 percent.

#### 17. Create (and output) 95% confidence intervals for $\beta_k$, $\exp\{\beta_k\}$, and $100 \times (\exp\{\beta_k\} - 1)%$ for all predictors using the `confint` function.

```{r, fig.align='center'}
# your code here

# Log odds
confint(dia_lm)

# Odds
exp(confint(dia_lm))

100 * (exp(confint(dia_lm)) - 1)
```

#### 18. Interpret the 95% confidence intervals for bmi for $\beta_{bmi}$, $\exp\{\beta_{bmi}\}$, and $100 \times (\exp\{\beta_{bmi}\} - 1)%$ (three interpretations total).

*Interpretation using $\beta_{bmi}$:* We are 95% confident that holding all else constant, the log odds of someone getting diabetes increases between (0.03390702,  0.11376823) for every unit increase in bmi.

*Interpretation using $\exp\{\beta_{bmi}\}$:* We are 95% confident that holding all else constant, the odds of someone getting diabetes is between (1.034488e+00, 1.1204923958) times more likely for every unit increase in bmi.

*Interpretation using $100 \times (\exp\{\beta_{bmi}\} - 1)%$:* We are 95% confident that holding all else constant, the odds of someone getting diabetes increases by between (3.448842, 12.049240) percent for every unit increase in bmi.

#### 19. Calculate a 95% confidence interval for the predicted probability that a patient has diabetes where pregnant = 1, glucose = 90, diastolic = 62, triceps = 18, insulin = 59, bmi = 25.1, pedigree = 1.268 and age = 25. Note that you may not need to use all of these values depending on the variables you chose to include in your model. *Do you think this patient will develop diabetes? Why or why not?*

```{r}
# your code here
new_patient <- data.frame(glucose = 90, bmi = 25.1, pedigree = 1.268, age = 25)

me <- qnorm(p=.975, lower.tail = TRUE) * predict(dia_lm, newdata=new_patient, se.fit = TRUE)$se.fit

ci_log_odds <- predict(dia_lm, newdata=new_patient, se.fit=TRUE)$fit + c(-1,0,1) * me

exp(ci_log_odds) / (1 + exp(ci_log_odds))

```

Based purely off the interval and not yet knowing the accuracy of this model, I would say that this person should not be worried about developing diabetes. The highest predicted probability on this interval is just under 20%, which is not very high relatively speaking.

#### 20. Compute the likelihood ratio test statistic (aka deviance, aka model chi-squared test) for the model, and compute the associated $p$-value. Print out the test statistic and the $p$-value. *Based on the results, what do you conclude?*

```{r, fig.align='center'}
# your code here

like_ratio = dia_lm$null.deviance - dia_lm$deviance
like_ratio

pchisq(q = like_ratio, df = length(dia_lm$coefficients) - 1,
       lower.tail = FALSE)


```

With a very small p-value, there is enough evidence the reject the null hypothesis and conclude that at least one of the predictors can significantly predict the outcome.

#### 21. Compute (and output) the pseudo $R^2$ value for the model.

```{r, fig.align='center'}
# your code here
1 - (dia_lm$deviance/dia_lm$null.deviance)
```

#### 22. What is the best cutoff value for the model that minimizes the percent misclassified? Show your code and output the best cutoff value.

```{r, fig.align='center'}
# your code here

preds <- predict(dia_lm, type = "response")
possible_cutoffs <- seq(0, 1, length = 100)
percent_misclass <- rep(NA, length(possible_cutoffs))

for(i in 1:length(possible_cutoffs)) {
  cutoff <- possible_cutoffs[i]  # (1)
  classify <- ifelse(preds > cutoff, 1, 0)  # (2) 
  percent_misclass[i] <- mean(classify != unfactor(dia$diabetes)) # (3)
}

misclass_data <- as.data.frame(cbind(percent_misclass, possible_cutoffs))

ggplot(data = misclass_data) +
  geom_line(mapping = aes(x = possible_cutoffs, y = percent_misclass),
            linewidth = 2) +
  theme_bw() + 
  xlab("Cutoff Value") +
  ylab("Percent Misclassified") +
  theme(aspect.ratio = 1)

cutoff <- possible_cutoffs[which.min(percent_misclass)]
cutoff
```


#### 23. Create (and output) a confusion matrix using the best cutoff value you found above.

```{r, fig.align='center'}
# your code here

preds_matrix = preds > cutoff
confusion_matrix = table('truth' = dia$diabetes,
                         'prediction' = preds_matrix)

confusion_matrix
#addmargins(confusion_matrix)
```

#### 24. Based on the confusion matrix, what is the value for the specificity, and what does the specificity measure? Print the specificity.

```{r, fig.align='center'}
# your code here
229/(229+32)

```

The specificity measures the percentage of correctly predicted No's among all of the No's.

#### 25. Based on the confusion matrix, what is the value for the sensitivity, and what does the sensitivity measure? Print the sensitivity.

```{r, fig.align='center'}
# your code here
86/(44+86)
```

The sensitivity meaures the percentage of correctly predicted Yes's among all Yes's.

#### 26. Based on the confusion matrix, what is the percent correctly classified (accuracy), and what does the percent correctly classified measure? Print the percent correctly classified.

```{r, fig.align='center'}

(229+86) / (229+86+32+44)
```

The percent correctly classified measures the percent of correctly predicted yes and no values over all of the predictions in total.

#### 27. Plot (and output) the ROC curve for the model (either using the `pROC` package or the `ROCR` package).

```{r, fig.align='center'}
# your code here
my_roc <- roc(dia$diabetes, preds)

ggplot() +
  geom_path(mapping = aes(x = 1 - my_roc$specificities, 
                          y = my_roc$sensitivities), 
            linewidth = 2) +
  geom_abline(slope = 1, intercept = 0) +
  theme_bw() + 
  xlab("1 - Specificity (False Positive Rate)") +
  ylab("Sensitivity (True Positive Rate)") +
  theme(aspect.ratio = 1)
```

#### 28. What is the AUC for the ROC curve plotted above? Print the value of the AUC.

```{r, fig.align='center'}
auc(my_roc)
```

#### 29. Briefly summarize what you learned, personally, from this analysis about the statistics, model fitting process, etc.

<span style="color:red"> I learned that it's really important to understand the context of your data before you pick a model. If I wasn't paying attention, maybe I would have tried to fit a OLS model to this data and it wouldn't have worked. </span>

#### 30. Briefly summarize what you learned from this analysis *to a non-statistician*. Write a few sentences about (1) the purpose of this data set and analysis and (2) what you learned about this data set from your analysis. Write your response as if you were addressing a business manager (avoid using statistics jargon) and just provide the main take-aways.

Using the data provided, we would like to create a model that can accurately detect diabetes so that treatment plans can be generated to alleviate complications. Based on the data, a model has been constructed that can detect diabetes with an accuracy of around 80%.


